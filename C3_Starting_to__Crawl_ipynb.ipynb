{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C3_Starting_to _Crawl.ipynb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM+jvGcPOsFKHWAO3lsVJ0o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steven1174/Web_Scraping_with_Python/blob/main/C3_Starting_to__Crawl_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "SG_6AFL1JaLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "pip install twitter"
      ],
      "metadata": {
        "id": "AhjvJqNmxRTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZ4zHf77JPkB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4 import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Twitter"
      ],
      "metadata": {
        "id": "uup9DHSgJgcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from twitter import Twitter"
      ],
      "metadata": {
        "id": "zI9b2pCpWNTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Twitter?"
      ],
      "metadata": {
        "id": "MOx_T3HHYXpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Twitter(auth=OAuth(<Access Token>, <Access Token Secret>, <Consumer Key>,<Consumer Secret>)))"
      ],
      "metadata": {
        "id": "W6v478duyONX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "html = urlopen(\"http://en.wikipedia.org/wiki/Kevin_Bacon\")\n",
        "bsObj = BeautifulSoup(html)\n",
        "\n",
        "for link in bsObj.findAll(\"a\"):\n",
        "  if 'href' in link.attrs:\n",
        "    print(link.attrs['href'])"
      ],
      "metadata": {
        "id": "cKpYgDFrJiu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "html = urlopen(\"http://en.wikipedia.org/wiki/Kevin_Bacon\")\n",
        "bsObj = BeautifulSoup(html)\n",
        "\n",
        "for link in bsObj.find(\"div\", {\"id\":\"bodyContent\"}).findAll(\"a\", href=re.compile(\"^(/wiki/)((?!:).)*$\")):\n",
        "  if 'href' in link.attrs:\n",
        "    print(link.attrs['href'])"
      ],
      "metadata": {
        "id": "-pGCkocXL6W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import random\n",
        "import re"
      ],
      "metadata": {
        "id": "yKVIoyguNoGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(datetime.datetime.now())\n",
        "\n",
        "def getLinks(articleUrl):\n",
        "  html = urlopen(\"http://en.wikipedia.org\" + articleUrl)\n",
        "  bsObj = BeautifulSoup(html)\n",
        "  return bsObj.find(\"div\", {\"id\":\"bodyContent\"}).findAll(\"a\", href=re.compile(\"^(/wiki/)((?!:).)*$\"))"
      ],
      "metadata": {
        "id": "XAQT7rm9Nrn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "links = getLinks(\"/wiki/Kevin_Bacon\")"
      ],
      "metadata": {
        "id": "DNmTz9eEOPF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# while len(links) > 0:\n",
        "#  newArticle = newArticle = links[random.randint(0, len(links)-1)].attrs[\"href\"]\n",
        "#  print(newArticle)\n",
        "#  links = getLinks(newArticle)"
      ],
      "metadata": {
        "id": "EcgdJF2dOYbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crawling an Entire Site"
      ],
      "metadata": {
        "id": "sG2JvzBLP_Zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pages = set()\n",
        "\n",
        "def getLinks(pageUrl):\n",
        "\n",
        "  global pages\n",
        "  html = urlopen(\"http://en.wikipedia.org\" + pageUrl)\n",
        "  bsObj = BeautifulSoup(html)\n",
        "\n",
        "  for link in bsObj.findAll(\"a\", href=re.compile(\"^(/wiki/)\")):\n",
        "    if 'href' in link.attrs:\n",
        "      if link.attrs['href'] not in pages:\n",
        "        newPage = link.attrs['href']\n",
        "        print(newPage)\n",
        "\n",
        "        pages.add(newPage)\n",
        "        getLinks(newPage)"
      ],
      "metadata": {
        "id": "FceEAaQbP-87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getLinks(\"\")"
      ],
      "metadata": {
        "id": "DDfPTQqhSbHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages = set()\n",
        "\n",
        "def getLinks(pageUrl):\n",
        "  global pages\n",
        "  html = urlopen(\"http://en.wikipedia.org\" + pageUrl)\n",
        "  bsObj = BeautifulSoup(html)\n",
        "\n",
        "  try:\n",
        "    print(bsObj.h1.get_text())\n",
        "    print(bsObj.find(id =\"mw-content-text\").findAll(\"p\")[0])\n",
        "    print(bsObj.find(id=\"ca-edit\").find(\"span\").find(\"a\").attrs['href'])\n",
        "  except AttributeError:\n",
        "    print(\"This page is missing something! No worries though!\")\n",
        "    \n",
        "  for link in bsObj.findAll(\"a\", href=re.compile(\"^(/wiki/)\")):\n",
        "    if 'href' in link.attrs:\n",
        "      if link.attrs['href'] not in pages:\n",
        "        newPage = link.attrs['href']\n",
        "        print(\"----------------\\n\"+newPage)\n",
        "        pages.add(newPage)\n",
        "        getLinks(newPage)"
      ],
      "metadata": {
        "id": "xzhgY5IzShN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getLinks(\"\")"
      ],
      "metadata": {
        "id": "BA3uShk7V2RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crawling Across the Internet"
      ],
      "metadata": {
        "id": "CYcL8f3mV2yX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import datetime\n",
        "import random"
      ],
      "metadata": {
        "id": "8WsNHHCSWeCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages = set()\n",
        "random.seed(datetime.datetime.now())\n",
        "\n",
        "#Retrieves a list of all Internal links found on a page\n",
        "def getInternalLinks(bsObj, includeUrl):\n",
        "\n",
        "  internalLinks = []\n",
        "\n",
        "  #Finds all links that begin with a \"/\"\n",
        "  for link in bsObj.findAll(\"a\", href=re.compile(\"^(/|.*\"+ includeUrl+ \")\")):\n",
        "    if link.attrs['href'] is not None:\n",
        "      if link.attrs['href'] not in internalLinks:\n",
        "        internalLinks.append(link.attrs['href'])\n",
        "  \n",
        "  return internalLinks\n",
        "\n",
        "#Retrieves a list of all external links found on a page\n",
        "def getExternalLinks(bsObj, excludeUrl):\n",
        "  \n",
        "  externalLinks = []\n",
        "\n",
        "  #Finds all links that start with \"http\" or \"www\" that do\n",
        "  #not contain the current URL\n",
        "  for link in bsObj.findAll(\"a\", href=re.compile(\"^(http|www)((?!\" + excludeUrl+ \").)*$\")):\n",
        "    if link.attrs['href'] is not None:\n",
        "      if link.attrs['href'] not in externalLinks:\n",
        "        externalLinks.append(link.attrs['href'])\n",
        "    \n",
        "  return externalLinks\n",
        "\n",
        "def splitAddress(address):\n",
        "  addressParts = address.replace(\"http://\", \"\").split(\"/\")\n",
        "  return addressParts\n",
        "\n",
        "def getRandomExternalLink(startingPage):\n",
        "  html = urlopen(startingPage)\n",
        "  bsObj = BeautifulSoup(html)\n",
        "  externalLinks = getExternalLinks(bsObj, splitAddress(startingPage)[0])\n",
        "  \n",
        "  if len(externalLinks) == 0:\n",
        "    internalLinks = getInternalLinks(startingPage)\n",
        "    return getNextExternalLink(internalLinks[random.randint(0,\n",
        "    len(internalLinks)-1)])\n",
        "  else:\n",
        "    return externalLinks[random.randint(0, len(externalLinks)-1)]\n",
        "\n",
        "def followExternalOnly(startingSite):\n",
        "  externalLink = getRandomExternalLink(\"http://oreilly.com\")\n",
        "  print(\"Random external link is: \"+externalLink)\n",
        "  followExternalOnly(externalLink)"
      ],
      "metadata": {
        "id": "sA5XQNeUUvng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# followExternalOnly(\"http://oreilly.com\")"
      ],
      "metadata": {
        "id": "y64eQzByXu-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Collects a list of all external URLs found on the site\n",
        "allExtLinks = set()\n",
        "allIntLinks = set()\n",
        "\n",
        "def getAllExternalLinks(siteUrl):\n",
        "  html = urlopen(siteUrl)\n",
        "  bsObj = BeautifulSoup(html)\n",
        "  internalLinks = getInternalLinks(bsObj,splitAddress(siteUrl)[0])\n",
        "  externalLinks = getExternalLinks(bsObj,splitAddress(siteUrl)[0])\n",
        "\n",
        "  for link in externalLinks:\n",
        "    if link not in allExtLinks:\n",
        "      allExtLinks.add(link)\n",
        "      print(link)\n",
        "\n",
        "  for link in internalLinks:\n",
        "    if link not in allIntLinks:\n",
        "      print(\"About to get link: \" + link)\n",
        "      allIntLinks.add(link)\n",
        "      getAllExternalLinks(link)"
      ],
      "metadata": {
        "id": "9XpWt0-8YQV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getAllExternalLinks(\"http://oreilly.com\")"
      ],
      "metadata": {
        "id": "cMKe7ZEcYevn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crawling with Scrapy"
      ],
      "metadata": {
        "id": "4P6qTfhRZeum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "pip install scrapy"
      ],
      "metadata": {
        "id": "kA8eOFsxar2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scrapy import Item, Field"
      ],
      "metadata": {
        "id": "jxGZB9YfZf42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Article(Item):\n",
        "  # define the fields for your item here like:\n",
        "  # name = scrapy.Field()\n",
        "  title = Field()"
      ],
      "metadata": {
        "id": "U6UTzQVVbA5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fTtBN6R5bE8N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}